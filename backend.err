[debate-score] Gemini API error: ApiError: {"error":{"code":429,"message":"You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_input_token_count, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 0, model: gemini-2.0-flash\nPlease retry in 13.0074749s.","status":"RESOURCE_EXHAUSTED","details":[{"@type":"type.googleapis.com/google.rpc.Help","links":[{"description":"Learn more about Gemini API quotas","url":"https://ai.google.dev/gemini-api/docs/rate-limits"}]},{"@type":"type.googleapis.com/google.rpc.QuotaFailure","violations":[{"quotaMetric":"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count","quotaId":"GenerateContentInputTokensPerModelPerMinute-FreeTier","quotaDimensions":{"location":"global","model":"gemini-2.0-flash"}},{"quotaMetric":"generativelanguage.googleapis.com/generate_content_free_tier_requests","quotaId":"GenerateRequestsPerMinutePerProjectPerModel-FreeTier","quotaDimensions":{"model":"gemini-2.0-flash","location":"global"}},{"quotaMetric":"generativelanguage.googleapis.com/generate_content_free_tier_requests","quotaId":"GenerateRequestsPerDayPerProjectPerModel-FreeTier","quotaDimensions":{"location":"global","model":"gemini-2.0-flash"}}]},{"@type":"type.googleapis.com/google.rpc.RetryInfo","retryDelay":"13s"}]}}
    at throwErrorIfNotOK (file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/@google/genai/dist/node/index.mjs:12330:30)
    at process.processTicksAndRejections (node:internal/process/task_queues:104:5)
    at async file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/@google/genai/dist/node/index.mjs:12033:13
    at async Models.generateContent (file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/@google/genai/dist/node/index.mjs:13434:24)
    at async file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/routes/debateScore.js:85:24 {
  status: 429
}
[debate-score] Groq API error: APIError: 413 {"error":{"message":"Request too large for model `llama-3.3-70b-versatile` in organization `org_01khxpz8dfekqse1bft8v2f0wr` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 13009, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing","type":"tokens","code":"rate_limit_exceeded"}}
    at APIError.generate (file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/groq-sdk/error.mjs:59:16)
    at Groq.makeStatusError (file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/groq-sdk/core.mjs:286:25)
    at Groq.makeRequest (file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/node_modules/groq-sdk/core.mjs:332:30)
    at process.processTicksAndRejections (node:internal/process/task_queues:104:5)
    at async file:///C:/Users/Vivekananda%20Reddy/PaperCast/backend/routes/debateScore.js:83:30 {
  status: 413,
  headers: {
    'alt-svc': 'h3=":443"; ma=86400',
    'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate',
    'cf-cache-status': 'DYNAMIC',
    'cf-ray': '9d0eb558ac26557b-HYD',
    connection: 'keep-alive',
    'content-length': '391',
    'content-type': 'application/json',
    date: 'Fri, 20 Feb 2026 14:31:34 GMT',
    'retry-after': '6',
    server: 'cloudflare',
    'set-cookie': '__cf_bm=bE6OMOstox5CnrQYKKrYFPJyixXqvglIGV8cGURgkz8-1771597894.5097623-1.0.1.1-Mf_NDoeatEYIHxUqa5qmcO9UHT2uhbqYTseT.WBexdVwuXxJYXchyYrbRzR44idIw0JkjXgRDc7LyvdPncSSicxQs3RTFWRex83sozfUUPeQXhpBWQkKfmv5Fu5CrcOj; HttpOnly; Secure; Path=/; Domain=groq.com; Expires=Fri, 20 Feb 2026 15:01:34 GMT',
    'strict-transport-security': 'max-age=15552000',
    vary: 'Origin',
    via: '1.1 google',
    'x-groq-region': 'bom',
    'x-ratelimit-limit-requests': '1000',
    'x-ratelimit-limit-tokens': '12000',
    'x-ratelimit-remaining-requests': '1000',
    'x-ratelimit-remaining-tokens': '12000',
    'x-ratelimit-reset-requests': '1ms',
    'x-ratelimit-reset-tokens': '1ms',
    'x-request-id': 'req_01khxqd4vsfb687sn2p9yd1qdr'
  },
  error: {
    error: {
      message: 'Request too large for model `llama-3.3-70b-versatile` in organization `org_01khxpz8dfekqse1bft8v2f0wr` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Requested 13009, please reduce your message size and try again. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing',
      type: 'tokens',
      code: 'rate_limit_exceeded'
    }
  }
}
Supabase error inserting session: {
  code: 'PGRST205',
  details: null,
  hint: null,
  message: "Could not find the table 'public.sessions' in the schema cache"
}
